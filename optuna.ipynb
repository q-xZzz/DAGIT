{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import optuna\n","from optuna.visualization import plot_optimization_history\n","from optuna.visualization import plot_param_importances\n","from sklearn.metrics import cross_val_score, roc_auc_score\n","from sklearn.model_selection import StratifiedKFold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.sparse import load_npz\n","X_train = load_npz('/data/processed_train.npz')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train = pd.read_csv('/data/train_essays.csv')\n","Y_train = train['label'].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kfold = StratifiedKFold(n_splits=5, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from xgboost import XGBClassifier\n","\n","def objective(trial):\n","    n_estimators = trial.suggest_int('n_estimators', 100, 2000)\n","    learning_rate = trial.suggest_float('learning_rate', 0.0001, 1)\n","    subsample = trial.suggest_float('subsample', 0.1, 1.0)\n","    colsample_bytree = trial.suggest_float('colsample_bytree', 0.1, 1.0)\n","    colsample_bylevel = trial.suggest_float('colsample_bylevel', 0.1, 1.0)\n","    reg_alpha = trial.suggest_float('reg_alpha', 0.0, 1.0)\n","    reg_lambda = trial.suggest_float('reg_lambda', 0.0, 1.0)\n","    \n","    xgb = XGBClassifier(n_estimators=n_estimators,\n","                        learning_rate=learning_rate,\n","                        subsample=subsample,\n","                        colsample_bytree=colsample_bytree,\n","                        colsample_bylevel=colsample_bylevel,\n","                        use_label_encoder=False,\n","                        reg_alpha=reg_alpha,\n","                        reg_lambda=reg_lambda,\n","                        random_state=42) \n","    \n","    score = cross_val_score(xgb, X_train, Y_train, cv=kfold, scoring='roc_auc', n_jobs=-1).mean()\n","    return score\n","\n","\n","xgb_study = optuna.create_study(direction='maximize')\n","xgb_study.optimize(objective, n_trials=100)\n","\n","# Plot optimization history\n","fig1 = plot_optimization_history(xgb_study)\n","fig1.show(config={\"staticPlot\": True})\n","\n","# Plot parameter importances\n","fig2 = plot_param_importances(xgb_study)\n","fig2.show(config={\"staticPlot\": True})\n","\n","xgb_best_params = xgb_study.best_params\n","xgb_best_score = xgb_study.best_value\n","\n","print('Xgboost Best score:', xgb_best_score)\n","print('Xgboost Best parameters:', xgb_best_params)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","\n","def objective(trial):\n","    max_iter = trial.suggest_int('max_iter', 500, 10000)\n","    alpha = trial.suggest_float('alpha', 1e-4, 10)  \n","    ridge = Ridge(solver='sag', max_iter=max_iter, tol=1e-4, alpha=alpha)\n","    score = cross_val_score(ridge, X_train, Y_train, cv=kfold, scoring='roc_auc', n_jobs=-1).mean()\n","    return score\n","\n","ridge_study = optuna.create_study(direction='maximize')\n","ridge_study.optimize(objective, n_trials=100)  \n","\n","\n","fig1 = plot_optimization_history(ridge_study)\n","fig1.show(config={\"staticPlot\": True})\n","\n","fig2 = plot_param_importances(ridge_study)\n","fig2.show(config={\"staticPlot\": True})\n","\n","ridge_best_params = ridge_study.best_params\n","ridge_best_score = ridge_study.best_value\n","\n","print('Ridge Best score:', ridge_best_score)\n","print('Ridge Best parameters:', ridge_best_params)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","def objective(trial):\n","    alpha = trial.suggest_float('alpha', 1e-5, 1)  \n","    mnb = MultinomialNB(alpha=alpha)\n","    score = cross_val_score(mnb, X_train, Y_train, cv=kfold, scoring='roc_auc', n_jobs=-1).mean()\n","    return score\n","\n","mnb_study = optuna.create_study(direction='maximize')\n","mnb_study.optimize(objective, n_trials=100)  \n","\n","fig1 = plot_optimization_history(mnb_study)\n","fig1.show(config={\"staticPlot\": True})\n","\n","fig2 = plot_param_importances(mnb_study)\n","fig2.show(config={\"staticPlot\": True})\n","\n","mnb_best_params = mnb_study.best_params\n","mnb_best_score = mnb_study.best_value\n","\n","print('MNB Best score:', mnb_best_score)\n","print('MNB Best parameters:', mnb_best_params)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","def objective(trial):\n","    C = trial.suggest_loguniform('C', 1e-6, 1e2)  \n","    \n","    svc = SVC(tol=1e-4, \n","              C=C, \n","              kernel='linear', \n","              gamma='auto', \n","              probability=True)\n","\n","    score = cross_val_score(svc, X_train, Y_train, cv=kfold, scoring='roc_auc', n_jobs=-1).mean()\n","\n","    return score\n","\n","svc_study = optuna.create_study(direction='maximize')\n","svc_study.optimize(objective, n_trials=100)\n","\n","\n","fig1 = plot_optimization_history(svc_study)\n","fig1.show(config={\"staticPlot\": True})\n","\n","fig2 = plot_param_importances(svc_study)\n","fig2.show(config={\"staticPlot\": True})\n","\n","svc_best_params = svc_study.best_params\n","svc_best_score = svc_study.best_value\n","\n","print('SVC Best score:', svc_best_score)\n","print('SVC Best parameters:', svc_best_params)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
